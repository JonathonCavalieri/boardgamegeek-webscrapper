{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import request\n",
    "from datetime import datetime\n",
    "from requests import codes\n",
    "from html2text import html2text\n",
    "from json import dumps, dump\n",
    "import sqlalchemy as sa\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class boardgamescraper(object): \n",
    "\n",
    "    def __init__(self, configLocation:str, setupDbConnection:bool):\n",
    "\n",
    "        with open(configLocation, \"r\") as inFile:\n",
    "            try:\n",
    "                self.config = yaml.safe_load(inFile)\n",
    "                logging.info('Loaded Config')\n",
    "            except yaml.YAMLError as exc:\n",
    "                logging.error(exc)\n",
    "\n",
    "        if setupDbConnection:\n",
    "            self.setupSQLServer(self.config['DB_driver'],self.config['DB_host'],self.config['DB_database'])\n",
    "        else:\n",
    "            self.SQLConnectionReady = False\n",
    "    \n",
    "    \"\"\" Following Function are for Webscraping boardgame website\"\"\"\n",
    "\n",
    "    def getBoardgameIds(self, page=1):\n",
    "        url = f'https://boardgamegeek.com/browse/boardgame/page/{page}'\n",
    "\n",
    "        response = request('GET',url)\n",
    "        if response.status_code == codes.ok:\n",
    "            parsed_html = BeautifulSoup(response.text, 'lxml')\n",
    "            data = []\n",
    "            for row in parsed_html.find_all( 'a',{\"class\": \"primary\"}):\n",
    "                dict = {}\n",
    "                dict['name']=row.text\n",
    "                id = re.split('/',row['href'])[2]\n",
    "                dict['objectid'] = id\n",
    "                dict['link']=row['href']\n",
    "                data.append(dict)\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def getBoardgameDetails(self, objectid):\n",
    "        params = {'nosession': 1, \n",
    "            'objectid': objectid,\n",
    "            'objecttype':'thing',\n",
    "            'subtype':'boardgame',\n",
    "            'type':'things'}\n",
    "\n",
    "        url = f'https://api.geekdo.com/api/geekitems'\n",
    "\n",
    "\n",
    "        response = request(\"GET\", url,params=params)\n",
    "        if response.status_code == codes.ok:\n",
    "            data = response.json()['item']\n",
    "            self.outputRawLocal(data,'dataDumps\\Details',objectid)\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def getBoardgameStats(self, objectid):\n",
    "        params = {'nosession': 1, \n",
    "            'objectid': objectid,\n",
    "            'objecttype':'thing',\n",
    "            'subtype':'boardgame',\n",
    "            'type':'things'}\n",
    "\n",
    "        url = f'https://api.geekdo.com/api/dynamicinfo'\n",
    "\n",
    "\n",
    "        response = request(\"GET\", url,params=params)\n",
    "        if response.status_code == codes.ok:\n",
    "            data = response.json()['item']\n",
    "            data['objectid'] = objectid\n",
    "            self.outputRawLocal(data,'dataDumps\\Stats',objectid)\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    \"\"\" Following Function are for Outputing data to different sources\"\"\"\n",
    "\n",
    "    def setupSQLServer(self, driver, server,database):\n",
    "        connection_string = (\n",
    "            f\"DRIVER={driver};\"\n",
    "            f\"SERVER={server};\"\n",
    "            f\"DATABASE={database};\"\n",
    "            r\"Trusted_Connection=Yes;\"\n",
    "        )\n",
    "        sqlalchemy_url = \"mssql+pyodbc:///?odbc_connect=\" + connection_string\n",
    "        self.engine = sa.create_engine(sqlalchemy_url, fast_executemany=True)\n",
    "        self.SQLConnectionReady = True\n",
    "    \n",
    "    def SQLOutputUpsert(self, df_update, targetTable, idColumns):\n",
    "        logging.info(f'Writing to {targetTable} table')\n",
    "        if not self.SQLConnectionReady:\n",
    "            logging.error('SQL Server connection not setup')\n",
    "            return False\n",
    "\n",
    "        sourceColumnNames = df_update.columns\n",
    "        with self.engine.begin() as conn:\n",
    "            #Get columns in table trying to write to\n",
    "            SQLREQUEST = conn.execute(sa.text(\"SELECT TOP 0 * from {}\".format(targetTable)))\n",
    "            targetColumnNames = [col[0] for col in SQLREQUEST.cursor.description]\n",
    "            \n",
    "            #find common columns between target table and source data and check id fields in \n",
    "            columns = list(set(targetColumnNames).intersection(sourceColumnNames))\n",
    "            if not columns: \n",
    "                logging.error('No common columns')\n",
    "                return False\n",
    "            \n",
    "            #check if id columns exist\n",
    "            idCheck = [x in columns for x in idColumns]\n",
    "            if False in idCheck:\n",
    "                logging.error('ID column not in Target/Source')\n",
    "                return False\n",
    "            #probably should put in error check to notify which columns mismatch\n",
    "            \n",
    "            try:\n",
    "                #Load Data into temporary table\n",
    "                df_update.to_sql(\"DataLoad\", conn, index=False, if_exists='replace')\n",
    "\n",
    "\n",
    "                #Create Update query based off columns\n",
    "                updateQuery = 'UPDATE t \\nSET'\n",
    "                # for each column ad a line to query to set it\n",
    "                for col in columns:\n",
    "                    updateQuery = updateQuery + '\\n\\tt.{0} = s.{0},'.format(col)\n",
    "                #Get rid of last comma and add on tabe/join section\n",
    "                updateQuery = updateQuery[:-1] + '\\nFROM {} t \\nJOIN DataLoad s ON '.format(targetTable)\n",
    "                #create join conditions for each of the ID fields\n",
    "                for col in idColumns:\n",
    "                    updateQuery = updateQuery + '\\n\\tt.{0} = s.{0} and'.format(col)\n",
    "                updateQuery = updateQuery[:-3] #Get rid of AND for last one\n",
    "\n",
    "\n",
    "                #create Insert query based off columns\n",
    "                #Get Columns to be updated\n",
    "                insertColumns = ''\n",
    "                for col in columns:\n",
    "                    insertColumns = insertColumns + '{},'.format(col)\n",
    "                insertColumns = insertColumns[:-1]\n",
    "                #Create Query\n",
    "                insertQuery = 'INSERT INTO {0} ({1}) \\n'\\\n",
    "                    'SELECT '\\\n",
    "                    '\\n\\t{1} '\\\n",
    "                    '\\nFROM DataLoad s '\\\n",
    "                    '\\nWHERE NOT EXISTS '\\\n",
    "                    '\\n\\t(SELECT NULL'\\\n",
    "                    '\\n\\t FROM {0} t'\\\n",
    "                    '\\n\\tWhere '.format(targetTable,insertColumns)\n",
    "                #add id conditions\n",
    "                for col in idColumns:\n",
    "                    insertQuery = insertQuery + '\\n\\t\\tt.{0} = s.{0} and'.format(col)\n",
    "                insertQuery = insertQuery[:-3] +')' #Get rid of AND for last one\n",
    "\n",
    "                mainQuery = \"\"\"\n",
    "                SET NOCOUNT ON;\n",
    "                DECLARE @rows_updated INT = 0;\n",
    "                DECLARE @rows_inserted INT = 0;\n",
    "                \n",
    "                {};\n",
    "                SELECT @rows_updated = @@ROWCOUNT;\n",
    "                \n",
    "                {};\n",
    "                SELECT @rows_inserted = @@ROWCOUNT;\n",
    "                \n",
    "                SELECT @rows_updated AS rows_updated, @rows_inserted AS rows_inserted;\n",
    "                \"\"\".format(updateQuery,insertQuery)\n",
    "                \n",
    "                #Run Query\n",
    "                result = conn.execute(sa.text(mainQuery)).fetchone()\n",
    "                logging.info(f\"{result[0]} row(s) updated, {result[1]} row(s) inserted\")\n",
    "            finally:\n",
    "                #drop the temporary table\n",
    "                conn.execute(sa.text(\"DROP TABLE IF EXISTS DataLoad\"))\n",
    "            \n",
    "            return True\n",
    "\n",
    "    def SQLOutput(self, df_update, targetTable, if_exists ='fail'):\n",
    "        logging.info(f'Writing to {targetTable} table')\n",
    "        if not self.SQLConnectionReady:\n",
    "            logging.error('SQL Server connection not setup')\n",
    "            return False\n",
    "\n",
    "        with self.engine.begin() as conn:\n",
    "            df_update.to_sql(targetTable, conn, index=False, if_exists=if_exists)\n",
    "\n",
    "    def outputRawLocal(self,data,location, name):\n",
    "        with open(f\"{location}\\{name}.json\", \"w\") as outfile:\n",
    "            dump(data, outfile)\n",
    "    \n",
    "    \"\"\" Following Function are for Parse response data\"\"\"\n",
    "\n",
    "    def ParseData(self, details, stats):\n",
    "        links = []\n",
    "        today = datetime.today()\n",
    "        for detail in details:\n",
    "            if 'links' in detail.keys():\n",
    "                link = detail['links']\n",
    "                link['gameid'] = detail['objectid']\n",
    "                links.append(link)\n",
    "\n",
    "        #parse the all the links for each board game \n",
    "        #this is tall data that has many descriptive things like developer/artist\n",
    "        boardgame_links =  pd.json_normalize(links, sep='_')\n",
    "        boardgame_links = boardgame_links.melt(var_name='link_type',id_vars='gameid')\n",
    "        boardgame_links = boardgame_links.explode('value',ignore_index=True)\n",
    "        boardgame_links.dropna(axis=0, subset =['value'], inplace=True)\n",
    "        boardgame_links.reset_index(drop=True,inplace=True)\n",
    "        links_expanded =  pd.DataFrame(boardgame_links['value'].values.tolist())\n",
    "        boardgame_links.drop(labels='value',axis=1,inplace=True)\n",
    "        boardgame_links = pd.concat([boardgame_links, links_expanded], axis=1)\n",
    "        boardgame_links = boardgame_links.convert_dtypes()\n",
    "\n",
    "        #Parse the response for the board game details\n",
    "        boardgame_details =  pd.json_normalize(details, sep='_',max_level=1)\n",
    "        #dict of columns to keep and the data types they should be\n",
    "        columns = {'objectid':'Int64', 'label':'string', 'href':'string',\n",
    "        'name':'string', 'yearpublished':'Int32', 'minplayers':'Int32', \n",
    "        'maxplayers':'Int32', 'minplaytime':'Int32', 'maxplaytime':'Int32',\n",
    "        'minage':'Int32', 'short_description':'string', 'description':'string',\n",
    "        'wiki':'string', 'imageurl':'string', 'topimageurl':'string',\n",
    "        'website_url':'string', 'website_title':'string', 'images_thumb':'string',\n",
    "        'images_square200':'string'}\n",
    "        columns = {k:v for k,v in columns.items() if k in boardgame_details.columns}\n",
    "        boardgame_details = boardgame_details[columns]\n",
    "        boardgame_details = boardgame_details.convert_dtypes()\n",
    "        boardgame_details = boardgame_details.astype(columns, errors='ignore')\n",
    "\n",
    "        #Clean up html tags in the some text fields\n",
    "        boardgame_details['wiki'] = boardgame_details.apply(lambda x: html2text(x['wiki']),axis=1)\n",
    "        boardgame_details['description'] = boardgame_details.apply(lambda x: html2text(x['description']),axis=1)\n",
    "\n",
    "\n",
    "        #Parse out the rank information from the stats data stream\n",
    "        columns_stats_rank = {'objectid':'Int64', 'rank':'Int32', 'baverage':'Float32'}\n",
    "        stats_rank = [x for x in stats if 'rankinfo' in x.keys()]\n",
    "        boardgame_stats_rank =  pd.json_normalize(stats_rank, sep='_', record_path='rankinfo', meta='objectid')\n",
    "        boardgame_stats_rank = boardgame_stats_rank.convert_dtypes()\n",
    "        boardgame_stats_rank = boardgame_stats_rank.astype(columns_stats_rank, errors='ignore')\n",
    "        \n",
    "\n",
    "        #parse out the user poll best player count from stats\n",
    "        stats_polls_best = [x for x in stats if 'best' in x['polls']['userplayers'].keys()]\n",
    "        boardgame_stats_polls_best =  pd.json_normalize(stats_polls_best, sep='_', record_path=['polls','userplayers','best'], meta='objectid')\n",
    "        columns_polls_best = {'min': 'players_best_min','max': 'players_best_max'}\n",
    "        boardgame_stats_polls_best.rename(columns=columns_polls_best, inplace=True)\n",
    "        boardgame_stats_polls_best.drop_duplicates(subset='objectid', inplace=True)\n",
    "        boardgame_stats_polls_best = boardgame_stats_polls_best.convert_dtypes()\n",
    "        boardgame_stats_polls_best = boardgame_stats_polls_best.astype({'objectid': 'Int64'}, errors='ignore')\n",
    "\n",
    "        #parse out the user poll recommended player count from stats\n",
    "        stats_polls_recommended = [x for x in stats if 'recommended' in x['polls']['userplayers'].keys()]\n",
    "        boardgame_stats_polls_recommended =  pd.json_normalize(stats_polls_recommended, sep='_', record_path=['polls','userplayers','recommended'], meta='objectid')\n",
    "        columns_polls_recommended = {'min': 'players_recommended_min','max': 'players_recommended_max'}\n",
    "        boardgame_stats_polls_recommended.rename(columns=columns_polls_recommended, inplace=True)\n",
    "        boardgame_stats_polls_recommended.drop_duplicates(subset='objectid', inplace=True)\n",
    "        boardgame_stats_polls_recommended = boardgame_stats_polls_recommended.convert_dtypes()\n",
    "        boardgame_stats_polls_recommended = boardgame_stats_polls_recommended.astype({'objectid': 'Int64'}, errors='ignore')\n",
    "\n",
    "        #parse the remaining stats data removing subobjects previously parsed\n",
    "        boardgame_stats =  pd.json_normalize(stats, sep='_')\n",
    "        drop_columns = ['rankinfo','commercelinks','polls_userplayers_best','polls_userplayers_recommended','has_ggs_link','polls_subdomain']\n",
    "        drop_columns = [x for x in drop_columns if x in boardgame_stats.columns]\n",
    "        boardgame_stats.drop(drop_columns,axis=1,inplace=True)\n",
    "        poll_columns = {x: re.sub('polls_','',x) for x in boardgame_stats.columns if 'polls_' in x}\n",
    "        stat_columns = {x: re.sub('stats_','',x) for x in boardgame_stats.columns if 'stats_' in x}\n",
    "        relatedcount_columnns = {x: re.sub('relatedcounts_','',x)+'_count' for x in boardgame_stats.columns if 'relatedcounts_' in x}\n",
    "        stats_column_renames = {**poll_columns, **stat_columns, **relatedcount_columnns}\n",
    "        boardgame_stats.rename(columns=stats_column_renames, inplace=True)\n",
    "\n",
    "        boardgame_stats = boardgame_stats.convert_dtypes()\n",
    "        boardgame_stats_dtypes = {'objectid': 'Int64', 'userplayers_totalvotes': 'Int32', \n",
    "        'boardgameweight_averageweight' : 'Float32', 'boardgameweight_votes': 'Int32',\n",
    "            'usersrated': 'Int32', 'average' : 'Float32', 'baverage' : 'Float32', 'stddev' : 'Float32', 'avgweight' : 'Float32',\n",
    "            'numweights': 'Int32', 'numgeeklists': 'Int32', 'numtrading': 'Int32', 'numwanting': 'Int32', 'numwish': 'Int32',\n",
    "            'numowned': 'Int32', 'numprevowned': 'Int32', 'numcomments': 'Int32', 'numwishlistcomments': 'Int32',\n",
    "            'numhasparts': 'Int32', 'numwantparts': 'Int32', 'views': 'Int32',  'numplays': 'Int32',\n",
    "            'numplays_month': 'Int32', 'numfans': 'Int32'}\n",
    "        boardgame_stats = boardgame_stats.astype(boardgame_stats_dtypes, errors='ignore')\n",
    "\n",
    "        #merge different sources that are at same level togeather \n",
    "        boardgame_stats = boardgame_stats.merge(boardgame_stats_polls_recommended, on='objectid', how='left', suffixes=(None, '_Right'))\n",
    "        boardgame_stats = boardgame_stats.merge(boardgame_stats_polls_best, on='objectid', how='left', suffixes=(None, '_Right'))\n",
    "        boardgame_details = boardgame_details.merge(boardgame_stats, on='objectid', how='left', suffixes=(None, '_Right'))\n",
    "\n",
    "        #add the updated date onto each dataframe\n",
    "        boardgame_details['updated_date'] = today\n",
    "        boardgame_links['updated_date'] = today\n",
    "        boardgame_stats_rank['updated_date'] = today\n",
    "        return boardgame_details, boardgame_links, boardgame_stats_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logs\\webscraper_{}.log'.format(datetime.today().strftime('%Y%m%d_%H%M%S')), filemode='w',level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "a_boardgamescraper= boardgamescraper('config.yaml', True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "\n",
    "\n",
    "id_list = a_boardgamescraper.getBoardgameIds(page)\n",
    "\n",
    "details =[]\n",
    "stats = []\n",
    "for row in id_list:\n",
    "    objectid = row['objectid']\n",
    "    logging.warning(f'Getting object id : {objectid}')\n",
    "    data_details = a_boardgamescraper.getBoardgameDetails(objectid)\n",
    "    if data_details:\n",
    "        details.append(data_details)\n",
    "    \n",
    "    data_stats = a_boardgamescraper.getBoardgameStats(objectid)\n",
    "    if data_stats:\n",
    "        stats.append(data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardgame_details, boardgame_links, boardgame_ranks = a_boardgamescraper.ParseData(details, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start from last time \n",
    "    logging.info(f'Read Page from file')\n",
    "    with open(\"lastPage\", \"r\") as inFile:\n",
    "        page = int(inFile.read())\n",
    "except:\n",
    "    logging.error(f'failed to read next page')\n",
    "\n",
    "page_original = page\n",
    "\n",
    "while page:\n",
    "    #get list of id from next page\n",
    "    logging.info(f'Staring scrapping of page:{page}')\n",
    "    id_list = a_boardgamescraper.getBoardgameIds(page)\n",
    "    \n",
    "    #for each id get the details and stats\n",
    "    details =[]\n",
    "    stats = []\n",
    "    for row in id_list:\n",
    "        objectid = row['objectid']\n",
    "        logging.info(f'Getting object id : {objectid}')\n",
    "        data_details = a_boardgamescraper.getBoardgameDetails(objectid)\n",
    "        if data_details:\n",
    "            details.append(data_details)\n",
    "        \n",
    "        data_stats = a_boardgamescraper.getBoardgameStats(objectid)\n",
    "        if data_stats:\n",
    "            stats.append(data_stats)\n",
    "\n",
    "    #parse all the data\n",
    "    boardgame_details, boardgame_links, boardgame_ranks = a_boardgamescraper.ParseData(details, stats)\n",
    "\n",
    "    a_boardgamescraper.SQLOutputUpsert(boardgame_details,'boardgame_details',['objectid'])\n",
    "    a_boardgamescraper.SQLOutputUpsert(boardgame_links,'boardgame_links',['gameid','objectid'])\n",
    "    a_boardgamescraper.SQLOutputUpsert(boardgame_ranks,'boardgame_ranks',['rankobjectid','objectid'])\n",
    "\n",
    "    page += 1\n",
    "    with open(\"lastPage\", \"w\") as outFile:\n",
    "        outFile.write(str(page))\n",
    "\n",
    "    if page > page_original + 400:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eab9e1de696d3886f357313d94a92a94bac4423ffcb6d01cb5ff4d4ac5162a1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('boardgameVENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
